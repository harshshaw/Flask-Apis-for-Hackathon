{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChatBot.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPhheOqL87BtTqTpv/stNIn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshshaw/Flask-Apis-for-Hackathon/blob/main/ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imHbwEenKl9q"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import string \n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJ--fRUHKoD6",
        "outputId": "af3858a9-378e-42ed-ade9-de7cd84656de"
      },
      "source": [
        "f=open('chatbot.txt','r',errors='ignore')\n",
        "raw_doc=f.read()\n",
        "raw_doc=raw_doc.lower()\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "sent_tokens=nltk.sent_tokenize(raw_doc)\n",
        "word_tokens=nltk.word_tokenize(raw_doc)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dpk7zCELgY9",
        "outputId": "b71b1f06-3dad-4267-d2b9-24d37de15d6c"
      },
      "source": [
        "sent_tokens[:2]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from noisy, structured and unstructured data,[1][2] and apply knowledge and actionable insights from data across a broad range of application domains.',\n",
              " 'data science is related to data mining, machine learning and big data.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GBXMzvTNFMn",
        "outputId": "51b2c45e-224c-4c19-b27d-55a73e1aa110"
      },
      "source": [
        "word_tokens[:2]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data', 'science']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPfHs4M8NLFa"
      },
      "source": [
        "lemmer=nltk.stem.WordNetLemmatizer()\n",
        "def LemToken(tokens):\n",
        "  return[lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict=dict((ord(punct),None)for punct in string.punctuation)\n",
        "def lemNormalize(text):\n",
        "  return LemToken(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Ij6gGWOHSi"
      },
      "source": [
        "GREET_INPUT=['hi','hello','sup','whats up','hey','greeting']\n",
        "GREET_RESPONSE=['hi','hey','hi there','hello','Im glad','You are talking to me']\n",
        "\n",
        "def greet(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in GREET_INPUT:\n",
        "      return random.choice(GREET_RESPONSE)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x7DOGjZaDEB"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MQv8x5HaOx9"
      },
      "source": [
        "def response(user_response):\n",
        "  robol_response=''\n",
        "  TfidfVec=TfidfVectorizer(tokenizer=lemNormalize,stop_words='english')\n",
        "  tfidf=TfidfVec.fit_transform(sent_tokens)\n",
        "  vals=cosine_similarity(tfidf[-1],tfidf)\n",
        "  idx=vals.argsort()[0][-2]\n",
        "  flat=vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf=flat[-2]\n",
        "  if(req_tfidf==0):\n",
        "    robol_response=robol_response+\"I am sorry i dont't understand you\"\n",
        "    return robol_response\n",
        "  else:\n",
        "    robol_response=robol_response+sent_tokens[idx]\n",
        "    return robol_response\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiuf-wlFz9AQ"
      },
      "source": [
        "\n",
        "class club():\n",
        "    def __init__():\n",
        "        flag=True\n",
        "        \n",
        "\n",
        "    def model():\n",
        "        flag=True\n",
        "        word_tokens=nltk.word_tokenize(raw_doc)\n",
        "        print(\"Bot:My name is Stark,Let's have a conversation! Also, if you want to exit any time,just type Bye!\")\n",
        "        while(flag==True):\n",
        "          user_response=input()\n",
        "          user_response=user_response.lower()\n",
        "          if(user_response!='bye'):\n",
        "            if(user_response=='thanks' or user_response=='thank you'):\n",
        "              flag=False\n",
        "              print(\"Bot:You are Welcome...\")\n",
        "            else :\n",
        "                if(greet(user_response)!=None):\n",
        "                  print(\"Bot:\"+greet(user_response))\n",
        "                else:\n",
        "                  sent_tokens.append(user_response)\n",
        "                  word_tokens+=nltk.word_tokenize(user_response)\n",
        "                  final_words=list(set(word_tokens))\n",
        "                  print(\"Bot:\",end=\"\")\n",
        "                  print(response(user_response))\n",
        "                  sent_tokens.remove(user_response)\n",
        "          else:\n",
        "            flag=False\n",
        "            print(\"Bot:Goodbye!Take care <3\")\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDPG8ZTb2bfL"
      },
      "source": [
        "call=club\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZCS1rdCPG4N",
        "outputId": "1592de4e-81a2-4d9b-b9d5-a9297aabf888"
      },
      "source": [
        "call.model()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot:My name is Stark,Let's have a conversation! Also, if you want to exit any time,just type Bye!\n",
            "science\n",
            "Bot:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "science\n",
            "data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot:\"data science\".\n",
            "wikipedia\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot:retrieved 3 april 2020.\n",
            "vte\n",
            "data\n",
            "categories: information sciencecomputer occupationscomputational fields of studydata analysis\n",
            "navigation menu\n",
            "not logged in\n",
            "talk\n",
            "contributions\n",
            "create account\n",
            "log in\n",
            "articletalk\n",
            "readeditview history\n",
            "search\n",
            "search wikipedia\n",
            "main page\n",
            "contents\n",
            "current events\n",
            "random article\n",
            "about wikipedia\n",
            "contact us\n",
            "donate\n",
            "contribute\n",
            "help\n",
            "learn to edit\n",
            "community portal\n",
            "recent changes\n",
            "upload file\n",
            "tools\n",
            "what links here\n",
            "related changes\n",
            "special pages\n",
            "permanent link\n",
            "page information\n",
            "cite this page\n",
            "wikidata item\n",
            "print/export\n",
            "download as pdf\n",
            "printable version\n",
            "in other projects\n",
            "wikimedia commons\n",
            "\n",
            "languages\n",
            "العربية\n",
            "বাংলা\n",
            "español\n",
            "हिन्दी\n",
            "bahasa indonesia\n",
            "русский\n",
            "தமிழ்\n",
            "اردو\n",
            "中文\n",
            "31 more\n",
            "edit links\n",
            "this page was last edited on 6 october 2021, at 10:48 (utc).\n",
            "bye\n",
            "Bot:Goodbye!Take care <3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "880aL-9xKqK0"
      },
      "source": [
        "import pickle\n",
        "pickle.dump(call,open('model1.pkl','wb'))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZC7dDpIaLSPF",
        "outputId": "3c4b74a8-5be0-4f2b-f8db-ca5db488807e"
      },
      "source": [
        "chat=pickle.load(open('model1.pkl','rb'))\n",
        "chat.model()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot:My name is Stark,Let's have a conversation! Also, if you want to exit any time,just type Bye!\n",
            "data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot:\"data science\".\n",
            "mathematics\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot:[3] it uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge.\n",
            "why?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot:I am sorry i dont't understand you\n",
            "calculate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot:I am sorry i dont't understand you\n",
            "statistics\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot:simply statistics.\n",
            "bye\n",
            "Bot:Goodbye!Take care <3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvuIpgJiLh5F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}